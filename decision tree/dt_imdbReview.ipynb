{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import shutil\n",
    "from urllib.request import urlopen\n",
    "from contextlib import closing\n",
    "\n",
    "#Data downloading \n",
    "\n",
    "#relative path of train/test data folder\n",
    "imdb_train_data_folder = \"./aclImdb/train\"\n",
    "imdb_test_data_folder = \"./aclImdb/test\"\n",
    "\n",
    "URL=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "ARCHIVE_NAME = \"aclImdb_v1.tar\"\n",
    "\n",
    "if not os.path.exists(\"aclImdb\"):\n",
    "    opener = urlopen(URL)\n",
    "    \n",
    "    #downloading and extract all files.\n",
    "    with open(ARCHIVE_NAME, 'wb') as archive:\n",
    "        archive.write(opener.read())\n",
    "        \n",
    "    with closing(tarfile.open(ARCHIVE_NAME, \"r:gz\")) as archive:\n",
    "        archive.extractall(path='.')\n",
    "        \n",
    "    test_folder = os.listdir(imdb_test_data_folder)\n",
    "    train_folder = os.listdir(imdb_train_data_folder)\n",
    "    \n",
    "    #remove .txt, .feat, and unsup folder.\n",
    "    for item in train_folder:\n",
    "        if (item.endswith(\".feat\") or item.endswith(\".txt\")):\n",
    "            os.remove(os.path.join(imdb_train_data_folder, item))\n",
    "    shutil.rmtree(os.path.join(imdb_train_data_folder,\"unsup\"))\n",
    "    for item in test_folder:\n",
    "        if (item.endswith(\".feat\") or item.endswith(\".txt\")):\n",
    "            os.remove(os.path.join(imdb_test_data_folder, item))\n",
    "    os.remove(ARCHIVE_NAME)\n",
    "#remove archieve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_samples: 25000\n",
      "test_samples: 25000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "#loads the data from folder\n",
    "imdb_train_data = load_files(imdb_train_data_folder,shuffle=False)\n",
    "print(\"train_samples: %d\" % len(imdb_train_data.data))\n",
    "\n",
    "imdb_test_data = load_files(imdb_test_data_folder,shuffle=False)\n",
    "print(\"test_samples: %d\" % len(imdb_test_data.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 74849)\n",
      "(25000, 74538)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Builds a dictionary of features and transforms \n",
    "#documents to feature vectors: \n",
    "count_vect= CountVectorizer()\n",
    "#second one exclude stopwords, like 'the','of'..\n",
    "count_vect2= CountVectorizer(stop_words='english') \n",
    "\n",
    "X_train_counts = count_vect.fit_transform(imdb_train_data.data)\n",
    "X_train_counts2 = count_vect2.fit_transform(imdb_train_data.data)\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "print(X_train_counts2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 74849)\n",
      "(25000, 74538)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#transform our count-matrix to a tf-idf representation\n",
    "#Similarly. a suffix 2 meaning we remove the stopwords\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer2 = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf2 = tfidf_transformer2.fit_transform(X_train_counts2)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_train_tfidf2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "# get the first document\n",
    "first_vector=X_train_tfidf[10]\n",
    "first_vector2=X_train_tfidf2[10]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree classifier part starts from below. RandomizedSearchCV is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Building a pipeline that behaves like a compound classifier\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "#text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('dt', tree.DecisionTreeClassifier())])\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('dt', tree.DecisionTreeClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n-gram range: (1, 2)\n",
      "Best stop_words: english\n",
      "Best use_idf: False\n",
      "Best criterion: gini\n",
      "Best max_depth: 45\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=45, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning using Randomized search\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "X = imdb_train_data.data\n",
    "Y = imdb_train_data.target\n",
    "# Create parameter for CountVectorizer\n",
    "ngram_range = [(1, 1), (1, 2), (2, 2)]\n",
    "# Create parameter for TfidfTransformer\n",
    "use_idf = (True, False)\n",
    "stop_words=['english', None]\n",
    "\n",
    "# Create lists of parameter for Decision Tree Classifier\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [45,65,95,125]\n",
    "\n",
    "\n",
    "n = len(X)\n",
    "idx1 = np.arange(0, n, dtype=int)\n",
    "idx1_train, idx1_test = train_test_split(idx1, test_size=0.2, shuffle = True, random_state = 123)\n",
    "custom_cv = [(idx1_train, idx1_test)]\n",
    "\n",
    "# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__â€™\n",
    "parameters = dict(vect__ngram_range=ngram_range,\n",
    "                  vect__stop_words=stop_words,\n",
    "                  tfidf__use_idf = use_idf,\n",
    "                      dt__criterion=criterion,\n",
    "                      dt__max_depth=max_depth);\n",
    "# Call RandomizedSearchCV\n",
    "rs_clf = RandomizedSearchCV(text_clf, parameters, n_iter=96, cv=custom_cv, random_state=199, return_train_score=False, \n",
    "                            n_jobs=-1)\n",
    "rs_clf.fit(X, Y)\n",
    "# View Best Parameters\n",
    "print('Best n-gram range:', rs_clf.best_estimator_.get_params()['vect__ngram_range'])\n",
    "print('Best stop_words:', rs_clf.best_estimator_.get_params()['vect__stop_words'])\n",
    "print('Best use_idf:', rs_clf.best_estimator_.get_params()['tfidf__use_idf'])\n",
    "print('Best criterion:', rs_clf.best_estimator_.get_params()['dt__criterion'])\n",
    "print('Best max_depth:', rs_clf.best_estimator_.get_params()['dt__max_depth'])\n",
    "print(); print(rs_clf.best_estimator_.get_params()['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision trees:\n",
      "0.72392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.74      0.69      0.71     12500\n",
      "         pos       0.71      0.76      0.73     12500\n",
      "\n",
      "    accuracy                           0.72     25000\n",
      "   macro avg       0.72      0.72      0.72     25000\n",
      "weighted avg       0.72      0.72      0.72     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see the prediction of the test set\n",
    "\n",
    "docs_test = imdb_test_data.data\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "print(\"Decision trees:\")\n",
    "predicted_dt1 = rs_clf.predict(docs_test)\n",
    "print(np.mean(predicted_dt1 == imdb_test_data.target))\n",
    "print(metrics.classification_report(imdb_test_data.target, predicted_dt1, target_names=imdb_test_data.target_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
