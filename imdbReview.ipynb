{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"imdbReview.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"haA9qmqUD3sq","colab_type":"code","colab":{}},"source":["import tarfile\n","import os\n","import shutil\n","from urllib.request import urlopen\n","from contextlib import closing\n","\n","#Data downloading\n","#relative path of train/test data folder\n","imdb_train_data_folder = \"./aclImdb/train\"\n","imdb_test_data_folder = \"./aclImdb/test\"\n","\n","URL=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n","ARCHIVE_NAME = \"aclImdb_v1.tar\"\n","\n","if not os.path.exists(\"aclImdb\"):\n","    opener = urlopen(URL)\n","    \n","    #downloading and extract all files.\n","    with open(ARCHIVE_NAME, 'wb') as archive:\n","        archive.write(opener.read())\n","        \n","    with closing(tarfile.open(ARCHIVE_NAME, \"r:gz\")) as archive:\n","        archive.extractall(path='.')\n","        \n","    test_folder = os.listdir(imdb_test_data_folder)\n","    train_folder = os.listdir(imdb_train_data_folder)\n","    \n","    #remove .txt, .feat, and unsup folder.\n","    for item in train_folder:\n","        if (item.endswith(\".feat\") or item.endswith(\".txt\")):\n","            os.remove(os.path.join(imdb_train_data_folder, item))\n","    shutil.rmtree(os.path.join(imdb_train_data_folder,\"unsup\"))\n","    for item in test_folder:\n","        if (item.endswith(\".feat\") or item.endswith(\".txt\")):\n","            os.remove(os.path.join(imdb_test_data_folder, item))\n","    os.remove(ARCHIVE_NAME)\n","#remove archieve"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xuJxaRsuD3su","colab_type":"code","outputId":"84576a42-9113-4e84-a838-728e2aae108b","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import sys\n","from sklearn.svm import LinearSVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.datasets import load_files\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","\n","#loads the data from folder\n","imdb_train_data = load_files(imdb_train_data_folder,shuffle=False)\n","print(\"train_samples: %d\" % len(imdb_train_data.data))\n","\n","imdb_test_data = load_files(imdb_test_data_folder,shuffle=False)\n","print(\"test_samples: %d\" % len(imdb_test_data.data))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train_samples: 25000\n","test_samples: 25000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ixE7o-fRD3sy","colab_type":"code","outputId":"54994c45-91f1-494d-ff68-2c999b43d10c","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","#Builds a dictionary of features and transforms \n","#documents to feature vectors: \n","count_vect= CountVectorizer()\n","#second one exclude stopwords, like 'the','of'..\n","count_vect2= CountVectorizer(stop_words='english') \n","\n","X_train_counts = count_vect.fit_transform(imdb_train_data.data)\n","X_train_counts2 = count_vect2.fit_transform(imdb_train_data.data)\n","\n","print(X_train_counts.shape)\n","print(X_train_counts2.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(25000, 74849)\n","(25000, 74538)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bm27Aml6D3s0","colab_type":"code","outputId":"f1c25655-0d66-491d-9773-063150d8c402","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","\n","#transform our count-matrix to a tf-idf representation\n","#Similarly. a suffix 2 meaning we remove the stopwords\n","tfidf_transformer = TfidfTransformer()\n","tfidf_transformer2 = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","X_train_tfidf2 = tfidf_transformer2.fit_transform(X_train_counts2)\n","\n","print(X_train_tfidf.shape)\n","print(X_train_tfidf2.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(25000, 74849)\n","(25000, 74538)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GIrAyPXLD3s3","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","# get the first document\n","first_vector=X_train_tfidf[10]\n","first_vector2=X_train_tfidf2[10]\n"," \n","# show the TF-IDF scores , compare with/without stopwords\n","df = pd.DataFrame(first_vector.T.todense(), index=count_vect.get_feature_names(), columns=[\"tfidf\"])\n","df2 = pd.DataFrame(first_vector2.T.todense(), index=count_vect2.get_feature_names(), columns=[\"tfidf_no_stopwords\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEMkivIfD3s6","colab_type":"code","outputId":"1c75ae86-4f1d-4104-9c0f-060a47e65fd4","colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["df.sort_values(by=[\"tfidf\"],ascending=False)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tfidf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>the</th>\n","      <td>0.252033</td>\n","    </tr>\n","    <tr>\n","      <th>he</th>\n","      <td>0.192326</td>\n","    </tr>\n","    <tr>\n","      <th>accent</th>\n","      <td>0.172488</td>\n","    </tr>\n","    <tr>\n","      <th>of</th>\n","      <td>0.167375</td>\n","    </tr>\n","    <tr>\n","      <th>three</th>\n","      <td>0.164270</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>flaw</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>flavourless</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>flavouring</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>flavoured</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>üvegtigris</th>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>74849 rows × 1 columns</p>\n","</div>"],"text/plain":["                tfidf\n","the          0.252033\n","he           0.192326\n","accent       0.172488\n","of           0.167375\n","three        0.164270\n","...               ...\n","flaw         0.000000\n","flavourless  0.000000\n","flavouring   0.000000\n","flavoured    0.000000\n","üvegtigris   0.000000\n","\n","[74849 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"y5hV29QID3s9","colab_type":"code","outputId":"84230c47-1510-4226-f76b-af40db73ce5b","colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["df2.sort_values(by=[\"tfidf_no_stopwords\"],ascending=False)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tfidf_no_stopwords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>accent</th>\n","      <td>0.214215</td>\n","    </tr>\n","    <tr>\n","      <th>spite</th>\n","      <td>0.169063</td>\n","    </tr>\n","    <tr>\n","      <th>actresses</th>\n","      <td>0.149836</td>\n","    </tr>\n","    <tr>\n","      <th>decrescendos</th>\n","      <td>0.147218</td>\n","    </tr>\n","    <tr>\n","      <th>bleibtreau</th>\n","      <td>0.147218</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>flavin</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>flavia</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>flava</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>flav</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>üvegtigris</th>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>74538 rows × 1 columns</p>\n","</div>"],"text/plain":["              tfidf_no_stopwords\n","accent                  0.214215\n","spite                   0.169063\n","actresses               0.149836\n","decrescendos            0.147218\n","bleibtreau              0.147218\n","...                          ...\n","flavin                  0.000000\n","flavia                  0.000000\n","flava                   0.000000\n","flav                    0.000000\n","üvegtigris              0.000000\n","\n","[74538 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qY0XhL1R9PUS"},"source":["# Models"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rvCR7PC69PUd"},"source":["## Logistic Regression"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-JyDQj259PUg","outputId":"56662300-dafd-4ca3-c258-c2b24b76af9c","colab":{"base_uri":"https://localhost:8080/","height":426}},"source":["from sklearn.pipeline import Pipeline\n","\n","from sklearn.linear_model import LogisticRegression\n","text_clf_lr = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf_lr', LogisticRegression(multi_class = 'multinomial')),])\n","text_clf_lr.fit(imdb_train_data.data, imdb_train_data.target)\n","# clf_lr = LogisticRegression().fit(X_train_tfidf2, twenty_train.target)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words='english', strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabular...\n","                ('tfidf',\n","                 TfidfTransformer(norm='l2', smooth_idf=True,\n","                                  sublinear_tf=False, use_idf=True)),\n","                ('clf_lr',\n","                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n","                                    fit_intercept=True, intercept_scaling=1,\n","                                    l1_ratio=None, max_iter=100,\n","                                    multi_class='multinomial', n_jobs=None,\n","                                    penalty='l2', random_state=None,\n","                                    solver='lbfgs', tol=0.0001, verbose=0,\n","                                    warm_start=False))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KN6umaKwbP4Q"},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"o72_gmcWbP4Z","outputId":"b736c8f7-434d-4752-f85e-10078158280b","colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["# Hyperparameter tuning using Randomized search\n","import numpy as np\n","import math\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n","X = imdb_train_data.data\n","Y = imdb_train_data.target\n","# Create parameter for CountVectorizer\n","ngram_range = [(1, 1), (1, 2), (2, 2)]\n","# Create parameter for TfidfTransformer\n","use_idf = (True, False)\n","\n","# Create lists of parameter for Logistic Regression Classifier:\n","penalty = ['l1', 'l2', 'elasticnet']\n","C = [0.001, 0.01, 0.1, 1, 10, 100]\n","class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n","class_weight.append(None)\n","solver = ['sag', 'saga']\n","\n","\n","n = len(X)\n","idx1 = np.arange(0, n, dtype=int)\n","idx1_train, idx1_test = train_test_split(idx1, test_size=0.2, shuffle = True, random_state = 123)\n","custom_cv = [(idx1_train, idx1_test)]\n","\n","# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__’\n","parameters = dict(vect__ngram_range=ngram_range,\n","                  tfidf__use_idf = use_idf, clf_lr__penalty = penalty, clf_lr__C = C, clf_lr__class_weight = class_weight, clf_lr__solver = solver);\n","# Call RandomizedSearchCV\n","rs_clf_lr = RandomizedSearchCV(text_clf_lr, parameters, n_iter=20, cv=custom_cv, random_state=199, return_train_score=False, n_jobs=-1)\n","rs_clf_lr.fit(X, Y)\n","\n","# View Best Parameters\n","print('Best n-gram range:', rs_clf_lr.best_estimator_.get_params()['vect__ngram_range'])\n","print('Best use_idf:', rs_clf_lr.best_estimator_.get_params()['tfidf__use_idf'])\n","print('Best penalty:', rs_clf_lr.best_estimator_.get_params()['clf_lr__penalty'])\n","print('Best C:', rs_clf_lr.best_estimator_.get_params()['clf_lr__C'])\n","print('Best class_weight:', rs_clf_lr.best_estimator_.get_params()['clf_lr__class_weight'])\n","print('Best solver:', rs_clf_lr.best_estimator_.get_params()['clf_lr__solver'])\n","print(); print(rs_clf_lr.best_estimator_.get_params()['clf_lr'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n"],"name":"stderr"},{"output_type":"stream","text":["Best n-gram range: (1, 2)\n","Best use_idf: False\n","Best penalty: l2\n","Best C: 10\n","Best class_weight: {1: 0.5, 0: 0.5}\n","Best solver: sag\n","\n","LogisticRegression(C=10, class_weight={0: 0.5, 1: 0.5}, dual=False,\n","                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n","                   max_iter=100, multi_class='multinomial', n_jobs=None,\n","                   penalty='l2', random_state=None, solver='sag', tol=0.0001,\n","                   verbose=0, warm_start=False)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cAY2F7NxEbH6","colab_type":"text"},"source":["### Evaluate"]},{"cell_type":"code","metadata":{"id":"Ln-Wa8xbEdkq","colab_type":"code","outputId":"a982dc0b-1875-4b42-bd1f-5362979eab8b","colab":{"base_uri":"https://localhost:8080/","height":212}},"source":["docs_test = imdb_test_data.data\n","\n","print(\"Logistic Regresssion:\")\n","predicted_lr = rs_clf_lr.predict(docs_test)\n","print(np.mean(predicted_lr == imdb_test_data.target))\n","from sklearn import metrics\n","print(metrics.classification_report(imdb_test_data.target, predicted_lr, target_names=imdb_test_data.target_names))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Logistic Regresssion:\n","0.8824\n","              precision    recall  f1-score   support\n","\n","         neg       0.88      0.88      0.88     12500\n","         pos       0.88      0.88      0.88     12500\n","\n","    accuracy                           0.88     25000\n","   macro avg       0.88      0.88      0.88     25000\n","weighted avg       0.88      0.88      0.88     25000\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z9_PRPOE9PUt"},"source":["## SVM\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VqTglvNP9PUu","outputId":"bacb8253-d4c3-4894-db73-de0d70a0b353","colab":{"base_uri":"https://localhost:8080/","height":372}},"source":["from sklearn.linear_model import SGDClassifier\n","text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf_svm', SGDClassifier()),])\n","text_clf_svm.fit(imdb_train_data.data, imdb_train_data.target)\n","# clf_svm = SGDClassifier().fit(X_train_tfidf2, twenty_train.target)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words='english', strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabular...\n","                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n","                               early_stopping=False, epsilon=0.1, eta0=0.0,\n","                               fit_intercept=True, l1_ratio=0.15,\n","                               learning_rate='optimal', loss='hinge',\n","                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n","                               penalty='l2', power_t=0.5, random_state=None,\n","                               shuffle=True, tol=0.001, validation_fraction=0.1,\n","                               verbose=0, warm_start=False))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"55UgiG7xtGgF"},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Pkj_HcKptGgH","outputId":"33534653-fce0-4ba1-8265-5f59bb84c517","colab":{"base_uri":"https://localhost:8080/","height":304}},"source":["# Create lists of parameter for SVM Classifier:\n","alpha = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n","learning_rate = ['optimal', 'constant', 'adaptive']\n","class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n","class_weight.append(None)\n","eta0 = [0.01, 0.1, 0.5, 0.75, 1] \n","\n","# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__’\n","parameters = dict(vect__ngram_range=ngram_range,\n","                  tfidf__use_idf = use_idf, clf_svm__alpha = alpha, clf_svm__learning_rate = learning_rate, clf_svm__class_weight = class_weight, clf_svm__eta0 = eta0);\n","# Call RandomizedSearchCV\n","rs_clf_svm = RandomizedSearchCV(text_clf_svm, parameters, n_iter=20, cv=custom_cv, random_state=199, return_train_score=False, n_jobs=-1)\n","rs_clf_svm.fit(X, Y)\n","\n","# View Best Parameters\n","print('Best n-gram range:', rs_clf_svm.best_estimator_.get_params()['vect__ngram_range'])\n","print('Best use_idf:', rs_clf_svm.best_estimator_.get_params()['tfidf__use_idf'])\n","print('Best alpha:', rs_clf_svm.best_estimator_.get_params()['clf_svm__alpha'])\n","print('Best learning_rate:', rs_clf_svm.best_estimator_.get_params()['clf_svm__learning_rate'])\n","print('Best class_weight:', rs_clf_svm.best_estimator_.get_params()['clf_svm__class_weight'])\n","print('Best eta0:', rs_clf_svm.best_estimator_.get_params()['clf_svm__eta0'])\n","print(); print(rs_clf_svm.best_estimator_.get_params()['clf_svm'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n"],"name":"stderr"},{"output_type":"stream","text":["Best n-gram range: (1, 2)\n","Best use_idf: True\n","Best alpha: 1e-05\n","Best learning_rate: constant\n","Best class_weight: None\n","Best eta0: 0.5\n","\n","SGDClassifier(alpha=1e-05, average=False, class_weight=None,\n","              early_stopping=False, epsilon=0.1, eta0=0.5, fit_intercept=True,\n","              l1_ratio=0.15, learning_rate='constant', loss='hinge',\n","              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n","              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n","              validation_fraction=0.1, verbose=0, warm_start=False)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nJqQsSR2EhFw","colab_type":"text"},"source":["### Evaluate"]},{"cell_type":"code","metadata":{"id":"nvDslgE4Ehy3","colab_type":"code","outputId":"5d23a9a0-5246-4202-bbbf-14cac45bfa89","colab":{"base_uri":"https://localhost:8080/","height":212}},"source":["print(\"SVM:\")\n","predicted_svm = rs_clf_svm.predict(docs_test)\n","print(np.mean(predicted_svm == imdb_test_data.target))\n","print(metrics.classification_report(imdb_test_data.target, predicted_svm, target_names=imdb_test_data.target_names))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["SVM:\n","0.88612\n","              precision    recall  f1-score   support\n","\n","         neg       0.89      0.88      0.89     12500\n","         pos       0.88      0.89      0.89     12500\n","\n","    accuracy                           0.89     25000\n","   macro avg       0.89      0.89      0.89     25000\n","weighted avg       0.89      0.89      0.89     25000\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vnT3naci9PU3"},"source":["## Random Forest"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tYwLcRRW9PU4","outputId":"49262477-186a-46e7-fee6-76a27f2892fb","colab":{"base_uri":"https://localhost:8080/","height":426}},"source":["from sklearn.ensemble import RandomForestClassifier\n","text_clf_rf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf_rf', RandomForestClassifier()),])\n","text_clf_rf.fit(imdb_train_data.data, imdb_train_data.target)\n","# clf_rf = RandomForestClassifier().fit(X_train_tfidf2, twenty_train.target)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words='english', strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabular...\n","                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n","                                        class_weight=None, criterion='gini',\n","                                        max_depth=None, max_features='auto',\n","                                        max_leaf_nodes=None, max_samples=None,\n","                                        min_impurity_decrease=0.0,\n","                                        min_impurity_split=None,\n","                                        min_samples_leaf=1, min_samples_split=2,\n","                                        min_weight_fraction_leaf=0.0,\n","                                        n_estimators=100, n_jobs=None,\n","                                        oob_score=False, random_state=None,\n","                                        verbose=0, warm_start=False))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"j5H6ClM2Aec1","colab_type":"text"},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5NQEDOxw9PU-","outputId":"e1a231b2-682b-4de7-9c4f-aaf970ab2ad0","colab":{"base_uri":"https://localhost:8080/","height":375}},"source":["# Create lists of parameter for Random Forest Classifier:\n","# Number of trees in random forest\n","n_estimators = [10, 100, 500, 750]\n","# Number of features to consider at every split\n","max_features = ['auto', 'sqrt']\n","# Maximum number of levels in tree\n","max_depth = [45, 65, 95, 125]\n","max_depth.append(None)\n","# Minimum number of samples required to split a node\n","min_samples_split = [2, 5, 10]\n","# Minimum number of samples required at each leaf node\n","min_samples_leaf = [1, 2, 4]\n","# Method of selecting samples for training each tree\n","bootstrap = [True, False]\n","\n","# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__’\n","parameters = dict(vect__ngram_range = ngram_range,\n","                  tfidf__use_idf = use_idf, clf_rf__n_estimators = n_estimators, clf_rf__max_features = max_features, clf_rf__max_depth=max_depth, clf_rf__min_samples_split = min_samples_split, clf_rf__min_samples_leaf = min_samples_leaf, clf_rf__bootstrap = bootstrap);\n","# Call RandomizedSearchCV\n","rs_clf_rf = RandomizedSearchCV(text_clf_rf, parameters, n_iter=20, cv=custom_cv, random_state=199, return_train_score=False, \n","                            n_jobs=-1)\n","rs_clf_rf.fit(X, Y)\n","\n","# View Best Parameters\n","print('Best n-gram range:', rs_clf_rf.best_estimator_.get_params()['vect__ngram_range'])\n","print('Best use_idf:', rs_clf_rf.best_estimator_.get_params()['tfidf__use_idf'])\n","print('Best n_estimators:', rs_clf_rf.best_estimator_.get_params()['clf_rf__n_estimators'])\n","print('Best max_features:', rs_clf_rf.best_estimator_.get_params()['clf_rf__max_features'])\n","print('Best max_depth:', rs_clf_rf.best_estimator_.get_params()['clf_rf__max_depth'])\n","print('Best min_samples_split:', rs_clf_rf.best_estimator_.get_params()['clf_rf__min_samples_split'])\n","print('Best min_samples_leaf:', rs_clf_rf.best_estimator_.get_params()['clf_rf__min_samples_leaf'])\n","print('Best bootstrap:', rs_clf_rf.best_estimator_.get_params()['clf_rf__bootstrap'])\n","print(); print(rs_clf_rf.best_estimator_.get_params()['clf_rf'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n"],"name":"stderr"},{"output_type":"stream","text":["Best n-gram range: (1, 2)\n","Best use_idf: False\n","Best n_estimators: 750\n","Best max_features: sqrt\n","Best max_depth: None\n","Best min_samples_split: 2\n","Best min_samples_leaf: 4\n","Best bootstrap: True\n","\n","RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=None, max_features='sqrt',\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=4, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=750,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eA1zHl_O9PVC"},"source":["### Evaluate"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"j_1KJsHs9PVE","outputId":"bcb5ec9f-8c1a-44f4-8b7f-bcc5b641ee00","colab":{"base_uri":"https://localhost:8080/","height":212}},"source":["print(\"Random Forest:\")\n","predicted_rf = rs_clf_rf.predict(docs_test)\n","print(np.mean(predicted_rf == imdb_test_data.target))\n","print(metrics.classification_report(imdb_test_data.target, predicted_rf, target_names=imdb_test_data.target_names))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Random Forest:\n","0.86868\n","              precision    recall  f1-score   support\n","\n","         neg       0.87      0.86      0.87     12500\n","         pos       0.86      0.88      0.87     12500\n","\n","    accuracy                           0.87     25000\n","   macro avg       0.87      0.87      0.87     25000\n","weighted avg       0.87      0.87      0.87     25000\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bfKH6iSAJOZf","colab_type":"text"},"source":["## AdaBoost"]},{"cell_type":"code","metadata":{"id":"lcb7sx02JZj0","colab_type":"code","outputId":"cd614afb-a05f-406e-ede1-4434f6104ecb","colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["from sklearn.pipeline import Pipeline\n","from sklearn import metrics\n","import numpy as np\n","\n","docs_test = imdb_test_data.data\n","\n","from sklearn.ensemble import AdaBoostClassifier\n","text_clf_ada = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf_ada', AdaBoostClassifier())])\n","text_clf_ada.fit(imdb_train_data.data, imdb_train_data.target)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabulary=None)),\n","                ('tfidf',\n","                 TfidfTransformer(norm='l2', smooth_idf=True,\n","                                  sublinear_tf=False, use_idf=True)),\n","                ('clf_ada',\n","                 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n","                                    learning_rate=1.0, n_estimators=50,\n","                                    random_state=None))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"D3IuQXapKkXS","colab_type":"code","outputId":"7fa29cd7-4ae8-484b-844c-4897df21a3c4","colab":{"base_uri":"https://localhost:8080/","height":649}},"source":["# Hyperparameter tuning using Randomized search\n","import math\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n","X = imdb_train_data.data\n","Y = imdb_train_data.target\n","# Create parameter for CountVectorizer\n","ngram_range = [(1, 1), (1, 2), (2, 2)]\n","stop_words=[]\n","# Create parameter for TfidfTransformer\n","use_idf = (True, False)\n","\n","n = len(X)\n","idx1 = np.arange(0, n, dtype=int)\n","idx1_train, idx1_test = train_test_split(idx1, test_size=0.2, shuffle = True, random_state = 123)\n","custom_cv = [(idx1_train, idx1_test)]\n","\n","#The maximum number of estimators at which boosting is terminated. \n","n_estimators=[50, 200, 400, 600]\n","\n","learning_rate=[0.001,0.1,0.3,1]\n","\n","#clf_ada__base_estimator = base_estimator\n","# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__’\n","parameters = dict(vect__ngram_range=ngram_range,\n","                  tfidf__use_idf = use_idf, clf_ada__n_estimators = n_estimators, clf_ada__learning_rate = learning_rate);\n","# Call RandomizedSearchCV\n","rs_clf_ada = RandomizedSearchCV(text_clf_ada, parameters, n_iter=20, cv=custom_cv, random_state=199, return_train_score=False, n_jobs=-1)\n","rs_clf_ada.fit(X, Y)\n","\n","# View Best Parameters\n","print('Best n-gram range:', rs_clf_ada.best_estimator_.get_params()['vect__ngram_range'])\n","print('Best use_idf:', rs_clf_ada.best_estimator_.get_params()['tfidf__use_idf'])\n","print('Best n_estimators:', rs_clf_ada.best_estimator_.get_params()['clf_ada__n_estimators'])\n","print('Best learning_rate:', rs_clf_ada.best_estimator_.get_params()['clf_ada__learning_rate'])\n","print(); print(rs_clf_ada.best_estimator_.get_params()['clf_ada'])\n","\n","print(rs_clf_ada)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n"],"name":"stderr"},{"output_type":"stream","text":["Best n-gram range: (1, 2)\n","Best use_idf: True\n","Best n_estimators: 600\n","Best learning_rate: 1\n","\n","AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n","                   n_estimators=600, random_state=None)\n","RandomizedSearchCV(cv=[(array([ 9064,  6051, 17848, ..., 17730, 15725, 19966]),\n","                        array([20000,  5515,   966, ..., 20230, 19078,  1073]))],\n","                   error_score=nan,\n","                   estimator=Pipeline(memory=None,\n","                                      steps=[('vect',\n","                                              CountVectorizer(analyzer='word',\n","                                                              binary=False,\n","                                                              decode_error='strict',\n","                                                              dtype=<class 'numpy.int64'>,\n","                                                              encoding='utf-8',\n","                                                              input='content',\n","                                                              lowercase=True,\n","                                                              max_df=1.0,\n","                                                              max_features=None...\n","                                                                 n_estimators=50,\n","                                                                 random_state=None))],\n","                                      verbose=False),\n","                   iid='deprecated', n_iter=20, n_jobs=-1,\n","                   param_distributions={'clf_ada__learning_rate': [0.001, 0.1,\n","                                                                   0.3, 1],\n","                                        'clf_ada__n_estimators': [50, 200, 400,\n","                                                                  600],\n","                                        'tfidf__use_idf': (True, False),\n","                                        'vect__ngram_range': [(1, 1), (1, 2),\n","                                                              (2, 2)]},\n","                   pre_dispatch='2*n_jobs', random_state=199, refit=True,\n","                   return_train_score=False, scoring=None, verbose=0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MyKnANNC1jnE","colab_type":"text"},"source":["### Evaluate"]},{"cell_type":"code","metadata":{"id":"C3AP1H_YzSOy","colab_type":"code","outputId":"89b6275c-d412-42b4-897d-43b1f1a72b61","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["print(\"AdaBoost:\")\n","predicted_ada = rs_clf_ada.predict(imdb_test_data.data)\n","print(np.mean(predicted_ada == imdb_test_data.target))\n","print(metrics.classification_report(imdb_test_data.target, predicted_ada, target_names=imdb_test_data.target_names))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["AdaBoost:\n","0.863\n","              precision    recall  f1-score   support\n","\n","         neg       0.88      0.85      0.86     12500\n","         pos       0.85      0.88      0.87     12500\n","\n","    accuracy                           0.86     25000\n","   macro avg       0.86      0.86      0.86     25000\n","weighted avg       0.86      0.86      0.86     25000\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6flcNAJh6uTw","colab_type":"text"},"source":["## Decision Tree"]},{"cell_type":"code","metadata":{"pycharm":{"is_executing":false},"id":"mkPyyqPjDeQV","colab_type":"code","colab":{}},"source":["# Building a pipeline that behaves like a compound classifier\n","from sklearn import tree\n","from sklearn.pipeline import Pipeline\n","#text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('dt', tree.DecisionTreeClassifier())])\n","text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('dt', tree.DecisionTreeClassifier())])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"393l4uYQ7pFM","colab_type":"text"},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"iqEkJpU1DeQY","colab_type":"code","outputId":"1302023a-8749-4154-a006-9a9159a620b9","colab":{}},"source":["# Hyperparameter tuning using Randomized search\n","import numpy as np\n","import math\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n","X = imdb_train_data.data\n","Y = imdb_train_data.target\n","# Create parameter for CountVectorizer\n","ngram_range = [(1, 1), (1, 2), (2, 2)]\n","# Create parameter for TfidfTransformer\n","use_idf = (True, False)\n","stop_words=['english', None]\n","\n","# Create lists of parameter for Decision Tree Classifier\n","criterion = ['gini', 'entropy']\n","max_depth = [45,65,95,125]\n","\n","\n","n = len(X)\n","idx1 = np.arange(0, n, dtype=int)\n","idx1_train, idx1_test = train_test_split(idx1, test_size=0.2, shuffle = True, random_state = 123)\n","custom_cv = [(idx1_train, idx1_test)]\n","\n","# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__’\n","parameters = dict(vect__ngram_range=ngram_range,\n","                  vect__stop_words=stop_words,\n","                  tfidf__use_idf = use_idf,\n","                      dt__criterion=criterion,\n","                      dt__max_depth=max_depth);\n","# Call RandomizedSearchCV\n","rs_clf = RandomizedSearchCV(text_clf, parameters, n_iter=96, cv=custom_cv, random_state=199, return_train_score=False, \n","                            n_jobs=-1)\n","rs_clf.fit(X, Y)\n","# View Best Parameters\n","print('Best n-gram range:', rs_clf.best_estimator_.get_params()['vect__ngram_range'])\n","print('Best stop_words:', rs_clf.best_estimator_.get_params()['vect__stop_words'])\n","print('Best use_idf:', rs_clf.best_estimator_.get_params()['tfidf__use_idf'])\n","print('Best criterion:', rs_clf.best_estimator_.get_params()['dt__criterion'])\n","print('Best max_depth:', rs_clf.best_estimator_.get_params()['dt__max_depth'])\n","print(); print(rs_clf.best_estimator_.get_params()['dt'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Best n-gram range: (1, 2)\n","Best stop_words: english\n","Best use_idf: False\n","Best criterion: gini\n","Best max_depth: 45\n","\n","DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n","                       max_depth=45, max_features=None, max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, presort='deprecated',\n","                       random_state=None, splitter='best')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"-NUqlSLaDeQb","colab_type":"code","outputId":"ae446698-b769-4cfb-a149-2b19864629b3","colab":{}},"source":["# Let's see the prediction of the test set\n","\n","docs_test = imdb_test_data.data\n","import numpy as np\n","from sklearn import metrics\n","print(\"Decision trees:\")\n","predicted_dt1 = rs_clf.predict(docs_test)\n","print(np.mean(predicted_dt1 == imdb_test_data.target))\n","print(metrics.classification_report(imdb_test_data.target, predicted_dt1, target_names=imdb_test_data.target_names))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Decision trees:\n","0.72392\n","              precision    recall  f1-score   support\n","\n","         neg       0.74      0.69      0.71     12500\n","         pos       0.71      0.76      0.73     12500\n","\n","    accuracy                           0.72     25000\n","   macro avg       0.72      0.72      0.72     25000\n","weighted avg       0.72      0.72      0.72     25000\n","\n"],"name":"stdout"}]}]}