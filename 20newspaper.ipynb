{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "20newspaper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9qQuHIVCgoM",
        "colab_type": "text"
      },
      "source": [
        "# 20 Newspaper dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbINtD25-l8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "90bf7b98-c805-49ea-e159-12c3f3a90624"
      },
      "source": [
        "#import 20newsgroups datasets from sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train',remove=(['headers','footers', 'quotes']))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvvb-DSM-l8t",
        "colab_type": "code",
        "outputId": "a9afe300-0d75-41c7-976a-820950b94c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#list of categories(labels)\n",
        "twenty_train.target_names"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPEhCp2s-l8x",
        "colab_type": "code",
        "outputId": "5dfbf257-bd82-403c-aff2-f63788bc6c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#counts the occurence of each word. \n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "#second one exclude stopwords, like 'the','of'..\n",
        "count_vect2= CountVectorizer(stop_words='english') \n",
        "\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "X_train_counts2 = count_vect2.fit_transform(twenty_train.data)\n",
        "\n",
        "print(X_train_counts.shape)\n",
        "print(X_train_counts2.shape)\n",
        "\n",
        "# count_vect.vocabulary_.get(u'algorithm')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 101631)\n",
            "(11314, 101322)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKBmDxM_-l80",
        "colab_type": "code",
        "outputId": "a2015dd8-0782-4c3c-fb19-90d8eaa1e5b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#transform from occurrences to tf-idf \n",
        "#tf-idf: 1. Decide the number of occurrences of each word in a document by total number of words in document. (Term Frequences tf)\n",
        "#        2. It does another refinement. Downscaling weights for words that occurring in many documents.\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidf_transformer2 = TfidfTransformer()\n",
        "\n",
        "#use X_train_tfidf to train the MODEL\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf2 = tfidf_transformer2.fit_transform(X_train_counts2)\n",
        "\n",
        "X_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
        "docs_test = X_test.data\n",
        "#use docs_test to fit.\n",
        "print(X_train_tfidf.shape)\n",
        "print(X_train_tfidf2.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 101631)\n",
            "(11314, 101322)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sESLkoq-l82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# get the first document\n",
        "first_vector=X_train_tfidf[13]\n",
        "first_vector2=X_train_tfidf2[13]\n",
        " \n",
        "# show the TF-IDF scores , compare with/without stopwords\n",
        "df = pd.DataFrame(first_vector.T.todense(), index=count_vect.get_feature_names(), columns=[\"tfidf\"])\n",
        "df2 = pd.DataFrame(first_vector2.T.todense(), index=count_vect2.get_feature_names(), columns=[\"tfidf_no_stopwords\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiPJkjsZ-l85",
        "colab_type": "code",
        "outputId": "2a7fa438-86d6-47b4-d579-dbae3cd397d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df.sort_values(by=[\"tfidf\"],ascending=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ssf</th>\n",
              "      <td>0.356347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flights</th>\n",
              "      <td>0.298625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>option</th>\n",
              "      <td>0.242772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>capability</th>\n",
              "      <td>0.242602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.216754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>discern</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>discarded</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>discard</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>discarcina</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ýé</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>101631 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               tfidf\n",
              "ssf         0.356347\n",
              "flights     0.298625\n",
              "option      0.242772\n",
              "capability  0.242602\n",
              "the         0.216754\n",
              "...              ...\n",
              "discern     0.000000\n",
              "discarded   0.000000\n",
              "discard     0.000000\n",
              "discarcina  0.000000\n",
              "ýé          0.000000\n",
              "\n",
              "[101631 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ITRIvQu-l88",
        "colab_type": "code",
        "outputId": "a8998ee2-7ba3-46d5-89f3-9d0956f5c032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df2.sort_values(by=[\"tfidf_no_stopwords\"],ascending=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tfidf_no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ssf</th>\n",
              "      <td>0.383239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flights</th>\n",
              "      <td>0.321161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>option</th>\n",
              "      <td>0.261093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>capability</th>\n",
              "      <td>0.260910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>module</th>\n",
              "      <td>0.224627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappoint</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappering</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappears</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappearing</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ýé</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>101322 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              tfidf_no_stopwords\n",
              "ssf                     0.383239\n",
              "flights                 0.321161\n",
              "option                  0.261093\n",
              "capability              0.260910\n",
              "module                  0.224627\n",
              "...                          ...\n",
              "disappoint              0.000000\n",
              "disappering             0.000000\n",
              "disappears              0.000000\n",
              "disappearing            0.000000\n",
              "ýé                      0.000000\n",
              "\n",
              "[101322 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9bzNbxc-8kB",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHggwwqW_FLy",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vm17pC-_HvM",
        "colab_type": "code",
        "outputId": "f73fc30f-9625-439f-e275-a29954413f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "text_clf_lr = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf_lr', LogisticRegression(multi_class = 'multinomial')),])\n",
        "text_clf_lr.fit(twenty_train.data, twenty_train.target)\n",
        "# clf_lr = LogisticRegression().fit(X_train_tfidf2, twenty_train.target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words='english', strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabular...\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf_lr',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='multinomial', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KN6umaKwbP4Q"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o72_gmcWbP4Z",
        "outputId": "f1ee4d60-745c-411c-988a-4a958af238f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Hyperparameter tuning using Randomized search\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "X = twenty_train.data\n",
        "Y = twenty_train.target\n",
        "# Create parameter for CountVectorizer\n",
        "ngram_range = [(1, 1), (1, 2), (2, 2)]\n",
        "# Create parameter for TfidfTransformer\n",
        "use_idf = (True, False)\n",
        "\n",
        "# Create lists of parameter for Logistic Regression Classifier:\n",
        "penalty = ['l1', 'l2', 'elasticnet']\n",
        "C = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
        "class_weight.append(None)\n",
        "solver = ['sag', 'saga']\n",
        "\n",
        "\n",
        "n = len(X)\n",
        "idx1 = np.arange(0, n, dtype=int)\n",
        "idx1_train, idx1_test = train_test_split(idx1, test_size=0.2, shuffle = True, random_state = 123)\n",
        "custom_cv = [(idx1_train, idx1_test)]\n",
        "\n",
        "# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__’\n",
        "parameters = dict(vect__ngram_range=ngram_range,\n",
        "                  tfidf__use_idf = use_idf, clf_lr__penalty = penalty, clf_lr__C = C, clf_lr__class_weight = class_weight, clf_lr__solver = solver);\n",
        "# Call RandomizedSearchCV\n",
        "rs_clf_lr = RandomizedSearchCV(text_clf_lr, parameters, n_iter=20, cv=custom_cv, random_state=199, return_train_score=False, n_jobs=-1)\n",
        "rs_clf_lr.fit(X, Y)\n",
        "\n",
        "# View Best Parameters\n",
        "print('Best n-gram range:', rs_clf_lr.best_estimator_.get_params()['vect__ngram_range'])\n",
        "print('Best use_idf:', rs_clf_lr.best_estimator_.get_params()['tfidf__use_idf'])\n",
        "print('Best penalty:', rs_clf_lr.best_estimator_.get_params()['clf_lr__penalty'])\n",
        "print('Best C:', rs_clf_lr.best_estimator_.get_params()['clf_lr__C'])\n",
        "print('Best class_weight:', rs_clf_lr.best_estimator_.get_params()['clf_lr__class_weight'])\n",
        "print('Best solver:', rs_clf_lr.best_estimator_.get_params()['clf_lr__solver'])\n",
        "print(); print(rs_clf_lr.best_estimator_.get_params()['clf_lr'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best n-gram range: (1, 1)\n",
            "Best use_idf: True\n",
            "Best penalty: l2\n",
            "Best C: 1\n",
            "Best class_weight: None\n",
            "Best solver: sag\n",
            "\n",
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='sag', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15k_tWnnAbil",
        "colab_type": "text"
      },
      "source": [
        "## SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ElSwwTD_Hy3",
        "colab_type": "code",
        "outputId": "6780d126-293c-4dbc-8715-3983780eca2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf_svm', SGDClassifier()),])\n",
        "text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
        "# clf_svm = SGDClassifier().fit(X_train_tfidf2, twenty_train.target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words='english', strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabular...\n",
              "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
              "                               fit_intercept=True, l1_ratio=0.15,\n",
              "                               learning_rate='optimal', loss='hinge',\n",
              "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "                               penalty='l2', power_t=0.5, random_state=None,\n",
              "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
              "                               verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "55UgiG7xtGgF"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e5ba05d4-6c34-46d3-b4c6-68c78acde6ac",
        "id": "Pkj_HcKptGgH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "# Create lists of parameter for SVM Classifier:\n",
        "alpha = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
        "learning_rate = ['optimal', 'constant', 'adaptive']\n",
        "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
        "class_weight.append(None)\n",
        "eta0 = [0.01, 0.1, 0.5, 1, 100] \n",
        "\n",
        "# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__’\n",
        "parameters = dict(vect__ngram_range=ngram_range,\n",
        "                  tfidf__use_idf = use_idf, clf_svm__alpha = alpha, clf_svm__learning_rate = learning_rate, clf_svm__class_weight = class_weight, clf_svm__eta0 = eta0);\n",
        "# Call RandomizedSearchCV\n",
        "rs_clf_svm = RandomizedSearchCV(text_clf_svm, parameters, n_iter=20, cv=custom_cv, random_state=199, return_train_score=False, n_jobs=-1)\n",
        "rs_clf_svm.fit(X, Y)\n",
        "\n",
        "# View Best Parameters\n",
        "print('Best n-gram range:', rs_clf_svm.best_estimator_.get_params()['vect__ngram_range'])\n",
        "print('Best use_idf:', rs_clf_svm.best_estimator_.get_params()['tfidf__use_idf'])\n",
        "print('Best alpha:', rs_clf_svm.best_estimator_.get_params()['clf_svm__alpha'])\n",
        "print('Best learning_rate:', rs_clf_svm.best_estimator_.get_params()['clf_svm__learning_rate'])\n",
        "print('Best class_weight:', rs_clf_svm.best_estimator_.get_params()['clf_svm__class_weight'])\n",
        "print('Best eta0:', rs_clf_svm.best_estimator_.get_params()['clf_svm__eta0'])\n",
        "print(); print(rs_clf_svm.best_estimator_.get_params()['clf_svm'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best n-gram range: (1, 2)\n",
            "Best use_idf: True\n",
            "Best alpha: 1e-05\n",
            "Best learning_rate: constant\n",
            "Best class_weight: None\n",
            "Best eta0: 0.5\n",
            "\n",
            "SGDClassifier(alpha=1e-05, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.5, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='constant', loss='hinge',\n",
            "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCY-lsNNBI8c",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7tSuPcD_H11",
        "colab_type": "code",
        "outputId": "0644551b-633c-4e46-e3ea-df11aadad224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "text_clf_rf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf_rf', RandomForestClassifier()),])\n",
        "text_clf_rf.fit(twenty_train.data, twenty_train.target)\n",
        "# clf_rf = RandomForestClassifier().fit(X_train_tfidf2, twenty_train.target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words='english', strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabular...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=None, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5H6ClM2Aec1",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liMfNqLa-85o",
        "colab_type": "code",
        "outputId": "e3d4f5da-468d-41cb-bd0d-f55796adfcaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "# Create lists of parameter for Random Forest Classifier:\n",
        "# Number of trees in random forest\n",
        "n_estimators = [10, 100, 500, 750]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [45, 65, 95, 125]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "\n",
        "# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__’\n",
        "parameters = dict(vect__ngram_range = ngram_range,\n",
        "                  tfidf__use_idf = use_idf, clf_rf__n_estimators = n_estimators, clf_rf__max_features = max_features, clf_rf__max_depth=max_depth, clf_rf__min_samples_split = min_samples_split, clf_rf__min_samples_leaf = min_samples_leaf, clf_rf__bootstrap = bootstrap);\n",
        "# Call RandomizedSearchCV\n",
        "rs_clf_rf = RandomizedSearchCV(text_clf_rf, parameters, n_iter=20, cv=custom_cv, random_state=199, return_train_score=False, \n",
        "                            n_jobs=-1)\n",
        "rs_clf_rf.fit(X, Y)\n",
        "\n",
        "# View Best Parameters\n",
        "print('Best n-gram range:', rs_clf_rf.best_estimator_.get_params()['vect__ngram_range'])\n",
        "print('Best use_idf:', rs_clf_rf.best_estimator_.get_params()['tfidf__use_idf'])\n",
        "print('Best n_estimators:', rs_clf_rf.best_estimator_.get_params()['clf_rf__n_estimators'])\n",
        "print('Best max_features:', rs_clf_rf.best_estimator_.get_params()['clf_rf__max_features'])\n",
        "print('Best max_depth:', rs_clf_rf.best_estimator_.get_params()['clf_rf__max_depth'])\n",
        "print('Best min_samples_split:', rs_clf_rf.best_estimator_.get_params()['clf_rf__min_samples_split'])\n",
        "print('Best min_samples_leaf:', rs_clf_rf.best_estimator_.get_params()['clf_rf__min_samples_leaf'])\n",
        "print('Best bootstrap:', rs_clf_rf.best_estimator_.get_params()['clf_rf__bootstrap'])\n",
        "print(); print(rs_clf_rf.best_estimator_.get_params()['clf_rf'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best n-gram range: (1, 2)\n",
            "Best use_idf: False\n",
            "Best n_estimators: 750\n",
            "Best max_features: sqrt\n",
            "Best max_depth: None\n",
            "Best min_samples_split: 2\n",
            "Best min_samples_leaf: 4\n",
            "Best bootstrap: True\n",
            "\n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='sqrt',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=4, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=750,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYU5OA4AD29O",
        "colab_type": "text"
      },
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvO-CJ1eDySn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "f6740fe3-23ce-4b10-d80e-838213d6ed7d"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "\n",
        "twenty_test = fetch_20newsgroups(subset='test', remove=(['headers','footers', 'quotes']))\n",
        "docs_test = twenty_test.data\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "text_clf_ada = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf_ada', AdaBoostClassifier())])\n",
        "text_clf_ada.fit(twenty_train.data, twenty_train.target)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf_ada',\n",
              "                 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
              "                                    learning_rate=1.0, n_estimators=50,\n",
              "                                    random_state=None))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebd0Gch0E368",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vgD78n8E26r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "15a58435-8a19-4271-ada5-62b8774d780a"
      },
      "source": [
        "# Hyperparameter tuning using Randomized search\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "X = twenty_train.data\n",
        "Y = twenty_train.target\n",
        "# Create parameter for CountVectorizer\n",
        "ngram_range = [(1, 1), (1, 2), (2, 2)]\n",
        "stop_words=[]\n",
        "# Create parameter for TfidfTransformer\n",
        "use_idf = (True, False)\n",
        "\n",
        "n = len(X)\n",
        "idx1 = np.arange(0, n, dtype=int)\n",
        "idx1_train, idx1_test = train_test_split(idx1, test_size=0.2, shuffle = True, random_state = 123)\n",
        "custom_cv = [(idx1_train, idx1_test)]\n",
        "\n",
        "#The maximum number of estimators at which boosting is terminated. \n",
        "n_estimators=[50, 100, 150, 200]\n",
        "\n",
        "learning_rate=[0.01,0.1,0.3,1]\n",
        "\n",
        "#clf_ada__base_estimator = base_estimator\n",
        "# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__’\n",
        "parameters = dict(vect__ngram_range=ngram_range,\n",
        "                  tfidf__use_idf = use_idf, clf_ada__n_estimators = n_estimators, clf_ada__learning_rate = learning_rate);\n",
        "# Call RandomizedSearchCV\n",
        "rs_clf_ada = RandomizedSearchCV(text_clf_ada, parameters, n_iter=20, cv=custom_cv, random_state=199, return_train_score=False, n_jobs=-1)\n",
        "rs_clf_ada.fit(X, Y)\n",
        "\n",
        "# View Best Parameters\n",
        "print('Best n-gram range:', rs_clf_ada.best_estimator_.get_params()['vect__ngram_range'])\n",
        "print('Best use_idf:', rs_clf_ada.best_estimator_.get_params()['tfidf__use_idf'])\n",
        "print('Best n_estimators:', rs_clf_ada.best_estimator_.get_params()['clf_ada__n_estimators'])\n",
        "print('Best learning_rate:', rs_clf_ada.best_estimator_.get_params()['clf_ada__learning_rate'])\n",
        "print(); print(rs_clf_ada.best_estimator_.get_params()['clf_ada'])\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best n-gram range: (1, 2)\n",
            "Best use_idf: False\n",
            "Best n_estimators: 150\n",
            "Best learning_rate: 0.3\n",
            "\n",
            "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.3,\n",
            "                   n_estimators=150, random_state=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViGBsJ2N4TRp",
        "colab_type": "text"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiONnKit4Ryk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building a pipeline that behaves like a compound classifier\n",
        "from sklearn import tree\n",
        "# Do the normalization of X_train_counts, and it could be used later on.\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "#text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('dt', tree.DecisionTreeClassifier())])\n",
        "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('norm', Normalizer()), ('dt', tree.DecisionTreeClassifier())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICPdrwcf5uBI",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHM25MVp5W-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "61f378c2-e559-46d6-84f1-e1867b7a50cb"
      },
      "source": [
        "# Hyperparameter tuning using Randomized search\n",
        "import numpy\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\n",
        "X = twenty_train.data\n",
        "Y = twenty_train.target\n",
        "# Create parameter for CountVectorizer\n",
        "ngram_range = [(1, 1), (1, 2), (2, 2)]\n",
        "# Create parameter for TfidfTransformer\n",
        "use_idf = (True, False)\n",
        "# Create lists of parameter for Decision Tree Classifier\n",
        "criterion = ['gini', 'entropy']\n",
        "max_depth = [45,65,95,125]\n",
        "#max_depth = [4,8,12]\n",
        "\n",
        "n = len(X)\n",
        "idx1 = numpy.arange(0, n, dtype=int)\n",
        "idx1_train, idx1_test = train_test_split(idx1, test_size=0.2, shuffle = True, random_state = 123)\n",
        "custom_cv = [(idx1_train, idx1_test)]\n",
        "\n",
        "# Create a dictionary of all the parameter options. We can access parameters of steps of a pipeline by using '__’\n",
        "parameters = dict(vect__ngram_range=ngram_range,\n",
        "                  tfidf__use_idf = use_idf,\n",
        "                      dt__criterion=criterion,\n",
        "                      dt__max_depth=max_depth);\n",
        "# Call RandomizedSearchCV\n",
        "rs_clf = RandomizedSearchCV(text_clf, parameters, n_iter=20, cv=custom_cv, random_state=199, return_train_score=False, \n",
        "                            n_jobs=-1)\n",
        "rs_clf.fit(X, Y)\n",
        "# View Best Parameters\n",
        "print('Best n-gram range:', rs_clf.best_estimator_.get_params()['vect__ngram_range'])\n",
        "print('Best use_idf:', rs_clf.best_estimator_.get_params()['tfidf__use_idf'])\n",
        "print('Best criterion:', rs_clf.best_estimator_.get_params()['dt__criterion'])\n",
        "print('Best max_depth:', rs_clf.best_estimator_.get_params()['dt__max_depth'])\n",
        "print(); print(rs_clf.best_estimator_.get_params()['dt'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best n-gram range: (1, 2)\n",
            "Best use_idf: False\n",
            "Best criterion: gini\n",
            "Best max_depth: 95\n",
            "\n",
            "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=95, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gdmeS142The",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation:\n",
        "\n",
        "*   Logistic Regression\n",
        "*   SVM\n",
        "*   Random Forest\n",
        "*   Adaboost\n",
        "*   Decision Tree\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwjxtWSb_uN5",
        "colab_type": "code",
        "outputId": "a9297dee-9d1e-4227-82e1-bbd286268054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "twenty_test = fetch_20newsgroups(subset='test', remove=(['headers','footers', 'quotes']))\n",
        "docs_test = twenty_test.data\n",
        "\n",
        "print(\"Logistic Regresssion:\")\n",
        "predicted_lr = rs_clf_lr.predict(docs_test)\n",
        "print(np.mean(predicted_lr == twenty_test.target))\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(twenty_test.target, predicted_lr, target_names=twenty_test.target_names))\n",
        "\n",
        "print(\"SVM:\")\n",
        "predicted_svm = rs_clf_svm.predict(docs_test)\n",
        "print(np.mean(predicted_svm == twenty_test.target))\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(twenty_test.target, predicted_svm, target_names=twenty_test.target_names))\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "predicted_rf = rs_clf_rf.predict(docs_test)\n",
        "print(np.mean(predicted_rf == twenty_test.target))\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(twenty_test.target, predicted_rf, target_names=twenty_test.target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regresssion:\n",
            "0.6909187466808284\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.49      0.45      0.47       319\n",
            "           comp.graphics       0.63      0.71      0.67       389\n",
            " comp.os.ms-windows.misc       0.66      0.63      0.64       394\n",
            "comp.sys.ibm.pc.hardware       0.68      0.65      0.66       392\n",
            "   comp.sys.mac.hardware       0.75      0.69      0.72       385\n",
            "          comp.windows.x       0.83      0.72      0.77       395\n",
            "            misc.forsale       0.76      0.79      0.77       390\n",
            "               rec.autos       0.75      0.71      0.73       396\n",
            "         rec.motorcycles       0.48      0.81      0.61       398\n",
            "      rec.sport.baseball       0.81      0.82      0.82       397\n",
            "        rec.sport.hockey       0.90      0.86      0.88       399\n",
            "               sci.crypt       0.89      0.67      0.76       396\n",
            "         sci.electronics       0.56      0.61      0.59       393\n",
            "                 sci.med       0.76      0.79      0.78       396\n",
            "               sci.space       0.70      0.75      0.73       394\n",
            "  soc.religion.christian       0.64      0.79      0.71       398\n",
            "      talk.politics.guns       0.59      0.67      0.63       364\n",
            "   talk.politics.mideast       0.85      0.75      0.79       376\n",
            "      talk.politics.misc       0.59      0.45      0.51       310\n",
            "      talk.religion.misc       0.59      0.22      0.31       251\n",
            "\n",
            "                accuracy                           0.69      7532\n",
            "               macro avg       0.70      0.68      0.68      7532\n",
            "            weighted avg       0.70      0.69      0.69      7532\n",
            "\n",
            "SVM:\n",
            "0.7035315985130112\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.55      0.50      0.52       319\n",
            "           comp.graphics       0.68      0.73      0.71       389\n",
            " comp.os.ms-windows.misc       0.64      0.62      0.63       394\n",
            "comp.sys.ibm.pc.hardware       0.66      0.65      0.66       392\n",
            "   comp.sys.mac.hardware       0.74      0.70      0.72       385\n",
            "          comp.windows.x       0.82      0.72      0.77       395\n",
            "            misc.forsale       0.74      0.80      0.77       390\n",
            "               rec.autos       0.78      0.71      0.74       396\n",
            "         rec.motorcycles       0.81      0.75      0.78       398\n",
            "      rec.sport.baseball       0.55      0.85      0.67       397\n",
            "        rec.sport.hockey       0.88      0.90      0.89       399\n",
            "               sci.crypt       0.83      0.72      0.77       396\n",
            "         sci.electronics       0.64      0.59      0.62       393\n",
            "                 sci.med       0.77      0.79      0.78       396\n",
            "               sci.space       0.75      0.76      0.76       394\n",
            "  soc.religion.christian       0.66      0.82      0.73       398\n",
            "      talk.politics.guns       0.59      0.68      0.63       364\n",
            "   talk.politics.mideast       0.85      0.76      0.80       376\n",
            "      talk.politics.misc       0.59      0.45      0.51       310\n",
            "      talk.religion.misc       0.47      0.31      0.38       251\n",
            "\n",
            "                accuracy                           0.70      7532\n",
            "               macro avg       0.70      0.69      0.69      7532\n",
            "            weighted avg       0.71      0.70      0.70      7532\n",
            "\n",
            "Random Forest:\n",
            "0.6570631970260223\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.54      0.29      0.38       319\n",
            "           comp.graphics       0.60      0.62      0.61       389\n",
            " comp.os.ms-windows.misc       0.61      0.65      0.63       394\n",
            "comp.sys.ibm.pc.hardware       0.62      0.64      0.63       392\n",
            "   comp.sys.mac.hardware       0.77      0.67      0.71       385\n",
            "          comp.windows.x       0.76      0.72      0.74       395\n",
            "            misc.forsale       0.73      0.75      0.74       390\n",
            "               rec.autos       0.68      0.68      0.68       396\n",
            "         rec.motorcycles       0.37      0.81      0.51       398\n",
            "      rec.sport.baseball       0.79      0.79      0.79       397\n",
            "        rec.sport.hockey       0.84      0.88      0.86       399\n",
            "               sci.crypt       0.81      0.71      0.76       396\n",
            "         sci.electronics       0.60      0.49      0.54       393\n",
            "                 sci.med       0.79      0.68      0.73       396\n",
            "               sci.space       0.76      0.72      0.74       394\n",
            "  soc.religion.christian       0.55      0.84      0.66       398\n",
            "      talk.politics.guns       0.56      0.68      0.62       364\n",
            "   talk.politics.mideast       0.84      0.75      0.79       376\n",
            "      talk.politics.misc       0.71      0.34      0.46       310\n",
            "      talk.religion.misc       0.63      0.07      0.12       251\n",
            "\n",
            "                accuracy                           0.66      7532\n",
            "               macro avg       0.68      0.64      0.64      7532\n",
            "            weighted avg       0.68      0.66      0.65      7532\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5crS4vlO_ipn",
        "colab_type": "text"
      },
      "source": [
        "Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_R3cSag2whV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "3d5d8b92-2752-497a-e5f6-37ff35b9fcb2"
      },
      "source": [
        "predicted_ada = rs_clf_ada.predict(twenty_test.data)\n",
        "print(\"Adaboost\")\n",
        "print(np.mean(predicted_ada == twenty_test.target))\n",
        "print(metrics.classification_report(twenty_test.target, predicted_ada, target_names=twenty_test.target_names))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adaboost\n",
            "0.4349442379182156\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.33      0.29      0.31       319\n",
            "           comp.graphics       0.50      0.46      0.48       389\n",
            " comp.os.ms-windows.misc       0.58      0.35      0.43       394\n",
            "comp.sys.ibm.pc.hardware       0.53      0.35      0.42       392\n",
            "   comp.sys.mac.hardware       0.72      0.41      0.52       385\n",
            "          comp.windows.x       0.74      0.44      0.55       395\n",
            "            misc.forsale       0.82      0.40      0.54       390\n",
            "               rec.autos       0.20      0.63      0.30       396\n",
            "         rec.motorcycles       0.88      0.44      0.58       398\n",
            "      rec.sport.baseball       0.46      0.56      0.51       397\n",
            "        rec.sport.hockey       0.85      0.38      0.53       399\n",
            "               sci.crypt       0.88      0.48      0.62       396\n",
            "         sci.electronics       0.18      0.50      0.26       393\n",
            "                 sci.med       0.30      0.51      0.38       396\n",
            "               sci.space       0.59      0.44      0.50       394\n",
            "  soc.religion.christian       0.64      0.48      0.55       398\n",
            "      talk.politics.guns       0.59      0.40      0.48       364\n",
            "   talk.politics.mideast       0.94      0.55      0.70       376\n",
            "      talk.politics.misc       0.23      0.37      0.28       310\n",
            "      talk.religion.misc       0.32      0.10      0.15       251\n",
            "\n",
            "                accuracy                           0.43      7532\n",
            "               macro avg       0.56      0.43      0.45      7532\n",
            "            weighted avg       0.57      0.43      0.46      7532\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YRJTWAL_Aql",
        "colab_type": "text"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkPeqEmC_D9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "340efdea-6880-4e44-c1e3-b9a8920a5e48"
      },
      "source": [
        "print(\"Decision Tree:\")\n",
        "predicted_dt = rs_clf.predict(twenty_test.data)\n",
        "print(np.mean(predicted_dt == twenty_test.target))\n",
        "print(metrics.classification_report(twenty_test.target, predicted_dt, target_names=twenty_test.target_names))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree:\n",
            "0.39312267657992567\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.35      0.18      0.24       319\n",
            "           comp.graphics       0.35      0.43      0.39       389\n",
            " comp.os.ms-windows.misc       0.55      0.42      0.48       394\n",
            "comp.sys.ibm.pc.hardware       0.43      0.26      0.33       392\n",
            "   comp.sys.mac.hardware       0.57      0.36      0.44       385\n",
            "          comp.windows.x       0.61      0.39      0.48       395\n",
            "            misc.forsale       0.63      0.52      0.57       390\n",
            "               rec.autos       0.14      0.66      0.24       396\n",
            "         rec.motorcycles       0.65      0.45      0.53       398\n",
            "      rec.sport.baseball       0.58      0.39      0.47       397\n",
            "        rec.sport.hockey       0.66      0.55      0.60       399\n",
            "               sci.crypt       0.72      0.42      0.53       396\n",
            "         sci.electronics       0.36      0.20      0.25       393\n",
            "                 sci.med       0.22      0.56      0.32       396\n",
            "               sci.space       0.65      0.40      0.50       394\n",
            "  soc.religion.christian       0.47      0.41      0.43       398\n",
            "      talk.politics.guns       0.43      0.33      0.37       364\n",
            "   talk.politics.mideast       0.81      0.45      0.58       376\n",
            "      talk.politics.misc       0.27      0.15      0.20       310\n",
            "      talk.religion.misc       0.22      0.13      0.16       251\n",
            "\n",
            "                accuracy                           0.39      7532\n",
            "               macro avg       0.48      0.38      0.40      7532\n",
            "            weighted avg       0.49      0.39      0.41      7532\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pseHcM-DNl3Q",
        "colab_type": "text"
      },
      "source": [
        "# Exploration: Data Preprocessing: \n",
        "*  Remove tags\n",
        "*  Lemmatization\n",
        "*  Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlTMtL_IOzS8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "4b51fbd9-5325-4ad2-97d5-a4ea3606b427"
      },
      "source": [
        "#unpreprocessed data\n",
        "print(twenty_train.data[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A fair number of brave souls who upgraded their SI clock oscillator have\n",
            "shared their experiences for this poll. Please send a brief message detailing\n",
            "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
            "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
            "functionality with 800 and 1.4 m floppies are especially requested.\n",
            "\n",
            "I will be summarizing in the next two days, so please add to the network\n",
            "knowledge base if you have done the clock upgrade and haven't answered this\n",
            "poll. Thanks.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os3Ou_NaR52R",
        "colab_type": "text"
      },
      "source": [
        "## Re: regex\n",
        "In Python, a regular expression is denoted as RE (REs, regexes or regex pattern) are embedded through re module.\n",
        "\"re\" module included with Python primarily used for string searching and manipulation.\n",
        "Also used frequently for webpage \"Scraping\" (extract large amount of data from websites).\n",
        "\n",
        "We use this package to remove the blank spaces, and br tags of the articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlGviMLqQirs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "Remove_space = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "Remove_br_tags = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "\n",
        "def preprocess(articles):\n",
        "    \n",
        "    #convert all words into lower cases. Remove \n",
        "    articles = [Remove_space.sub(\"\", line.lower()) for line in articles]\n",
        "    articles = [Remove_br_tags.sub(\" \", line) for line in articles]   \n",
        "    return articles\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brFvB1jIRG5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "9270d422-1f0d-4dce-8ac2-157dbec31f7e"
      },
      "source": [
        "#preprocessing the train/test data.\n",
        "twenty_train_clean = preprocess(twenty_train.data)\n",
        "twenty_test_clean = preprocess(twenty_test.data)\n",
        "\n",
        "print(\"Preprocessed Data:\")\n",
        "#print one preprocessed instance\n",
        "print(twenty_train_clean[1])\n",
        "print()\n",
        "print()\n",
        "print(\"Original Data:\")\n",
        "#unpreprocessed data\n",
        "print(twenty_train.data[1])\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessed Data:\n",
            "a fair number of brave souls who upgraded their si clock oscillator have\n",
            "shared their experiences for this poll please send a brief message detailing\n",
            "your experiences with the procedure top speed attained cpu rated speed\n",
            "add on cards and adapters heat sinks hour of usage per day floppy disk\n",
            "functionality with 800 and 14 m floppies are especially requested\n",
            "\n",
            "i will be summarizing in the next two days so please add to the network\n",
            "knowledge base if you have done the clock upgrade and havent answered this\n",
            "poll thanks\n",
            "\n",
            "\n",
            "Original Data:\n",
            "A fair number of brave souls who upgraded their SI clock oscillator have\n",
            "shared their experiences for this poll. Please send a brief message detailing\n",
            "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
            "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
            "functionality with 800 and 1.4 m floppies are especially requested.\n",
            "\n",
            "I will be summarizing in the next two days, so please add to the network\n",
            "knowledge base if you have done the clock upgrade and haven't answered this\n",
            "poll. Thanks.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBwGqplpTmLA",
        "colab_type": "text"
      },
      "source": [
        "## Lemmanization\n",
        "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. We would use the lemmatizer method from the nltk package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIRa8H_7WOiM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b401ea38-be7a-4b0e-f963-16f533245d95"
      },
      "source": [
        "#download packages.\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psa5VdF2TdHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lemmenized text.\n",
        "def Lemmanization(corpus):\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
        "\n",
        "lemmatized_twenty_train = Lemmanization(twenty_train_clean)\n",
        "lemmatized_twenty_test = Lemmanization(twenty_test_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2mmdH1FXxT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "921d47b2-42ae-4cb3-cc77-8672e4eebf01"
      },
      "source": [
        "print(lemmatized_twenty_train[1])\n",
        "print()\n",
        "print(twenty_train_clean[1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a fair number of brave soul who upgraded their si clock oscillator have shared their experience for this poll please send a brief message detailing your experience with the procedure top speed attained cpu rated speed add on card and adapter heat sink hour of usage per day floppy disk functionality with 800 and 14 m floppy are especially requested i will be summarizing in the next two day so please add to the network knowledge base if you have done the clock upgrade and havent answered this poll thanks\n",
            "\n",
            "a fair number of brave souls who upgraded their si clock oscillator have\n",
            "shared their experiences for this poll please send a brief message detailing\n",
            "your experiences with the procedure top speed attained cpu rated speed\n",
            "add on cards and adapters heat sinks hour of usage per day floppy disk\n",
            "functionality with 800 and 14 m floppies are especially requested\n",
            "\n",
            "i will be summarizing in the next two days so please add to the network\n",
            "knowledge base if you have done the clock upgrade and havent answered this\n",
            "poll thanks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq_eMAF6aznM",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessed data + Lemmanization Performance\n",
        "Now we will use the processed train/test data to see if it can improve the test accuracy.\n",
        "Instead of using GridSearch/RandomisedSearch CV, we would just use the hyperparameters trained in previous models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo5Plyr6djyp",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "evaluation with processed data/original data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLVu5JUSnocf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4f89c076-0f9d-4756-bb13-9c4a24013917"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "\n",
        "text_clf_lr2 = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer(use_idf=True)), ('clf_lr2', LogisticRegression())])\n",
        "text_clf_lr2.fit(lemmatized_twenty_train, twenty_train.target)\n",
        "\n",
        "text_clf_lr3 = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer(use_idf=True)), ('clf_lr3', LogisticRegression())])\n",
        "text_clf_lr3.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "print(\"LR:\")\n",
        "#Predict on clean and preprocess, remove tags, lowering words.  \n",
        "print(\"After preprocessing\")  \n",
        "predicted_lr2 = text_clf_lr2.predict(lemmatized_twenty_test)\n",
        "print(np.mean(predicted_lr2 == twenty_test.target))\n",
        "\n",
        "#Predict on original test sets. Twenty train  \n",
        "print(\"No preprocessing(default)\") \n",
        "predicted_lr3 = text_clf_lr3.predict(twenty_test.data)\n",
        "print(np.mean(predicted_lr3 == twenty_test.target))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR:\n",
            "After preprocessing\n",
            "0.6830855018587361\n",
            "No preprocessing(default)\n",
            "0.6909187466808284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDJ2TatDvjeZ",
        "colab_type": "text"
      },
      "source": [
        "### Adaboost\n",
        "evaluation with processed data/original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPGdhwmDTk0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8dd01db8-2241-4aa4-e346-02e458707e99"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "\n",
        "text_clf_ada2 = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer(use_idf=True)), ('clf_ada2', AdaBoostClassifier())])\n",
        "text_clf_ada2.fit(lemmatized_twenty_train, twenty_train.target)\n",
        "\n",
        "text_clf_ada3 = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer(use_idf=True)), ('clf_ada3', AdaBoostClassifier())])\n",
        "text_clf_ada3.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "print(\"Adaboost:\")\n",
        "#Predict on clean and preprocess, remove tags, lowering words.  \n",
        "print(\"After preprocessing\")  \n",
        "predicted_ada2 = text_clf_ada2.predict(lemmatized_twenty_test)\n",
        "print(np.mean(predicted_ada2 == twenty_test.target))\n",
        "\n",
        "#Predict on original test sets. Twenty train  \n",
        "print(\"No preprocessing(default)\") \n",
        "predicted_ada3 = text_clf_ada3.predict(twenty_test.data)\n",
        "print(np.mean(predicted_ada3 == twenty_test.target))\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adaboost:\n",
            "After preprocessing\n",
            "0.37958045671800317\n",
            "No preprocessing(default)\n",
            "0.36563993627190655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9D45tg7xGM1",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Classifier\n",
        "evaluation with processed data/original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjK2fK3ExNgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "56476a3a-e6e6-4c15-a387-e812baaf3d82"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "\n",
        "text_clf_rf2 = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer(use_idf=True)), ('clf_rf2', RandomForestClassifier())])\n",
        "text_clf_rf2.fit(lemmatized_twenty_train, twenty_train.target)\n",
        "\n",
        "text_clf_rf3 = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer(use_idf=True)), ('clf_rf3', RandomForestClassifier())])\n",
        "text_clf_rf3.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "print(\"Random Forest Classifier:\")\n",
        "#Predict on clean and preprocess, remove tags, lowering words.  \n",
        "print(\"After preprocessing\")  \n",
        "predicted_rf2 = text_clf_rf2.predict(lemmatized_twenty_test)\n",
        "print(np.mean(predicted_rf2 == twenty_test.target))\n",
        "\n",
        "#Predict on original test sets. Twenty train  \n",
        "print(\"No preprocessing(default)\") \n",
        "predicted_rf3 = text_clf_rf3.predict(twenty_test.data)\n",
        "print(np.mean(predicted_rf3 == twenty_test.target))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest Classifier:\n",
            "After preprocessing\n",
            "0.6257302177376527\n",
            "No preprocessing(default)\n",
            "0.6263940520446096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8BekZSayqTV",
        "colab_type": "text"
      },
      "source": [
        "### SGDClassifier\n",
        "evaluation with processed data/original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBnO2M4Dy6QY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "60173d05-764b-41c1-be7a-34e653f14b21"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "text_clf_sgd2 = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer(use_idf=True)), ('clf_sgd2', SGDClassifier())])\n",
        "text_clf_sgd2.fit(lemmatized_twenty_train, twenty_train.target)\n",
        "\n",
        "text_clf_sgd3 = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer(use_idf=True)), ('clf_sgd3', SGDClassifier())])\n",
        "text_clf_sgd3.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "print(\"SGDClassifier:\")\n",
        "#Predict on clean and preprocess, remove tags, lowering words.  \n",
        "print(\"After preprocessing\")  \n",
        "predicted_sgd2 = text_clf_sgd2.predict(lemmatized_twenty_test)\n",
        "print(np.mean(predicted_sgd2 == twenty_test.target))\n",
        "\n",
        "#Predict on original test sets. Twenty train  \n",
        "print(\"No preprocessing(default)\") \n",
        "predicted_sgd3 = text_clf_sgd3.predict(twenty_test.data)\n",
        "print(np.mean(predicted_sgd3 == twenty_test.target))\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGDClassifier:\n",
            "After preprocessing\n",
            "0.6964949548592672\n",
            "No preprocessing(default)\n",
            "0.6970260223048327\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}